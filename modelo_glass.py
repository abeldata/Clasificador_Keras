# -*- coding: utf-8 -*-
"""FinalProject_b_UD4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GRV54re7iAGPJ3Ng_PMzjkZKg4cPT41B

# Proyecto UD4
#### UD4. Redes Neuronais
#### MP. Sistemas de Aprendizaxe Automáticos
#### IES de Teis (Vigo), Cristina Gómez Alonso

Entrega en el moodle el fichero notebook solución así como un pdf generado con el contenido del notebook y su ejecución.

### Datasets de Clasificación: Cristales

#### Dataset Glass

Disponemos de un dataset con 214 instancias clasificadas en 7 tipos diferentes de cristales.

Lista de atributos para cada entrada será:

* 1. id: valor del 1 to 214
* 2. idx_refraccion: índice de refracción
* 3. sodio  (unidad de medida: porcentaje en peso del elemento (igual en el resto de atributos del 4 al 10))
* 4. magnesio
* 5. aluminio
* 6. silicio
* 7. potasio
* 8. calcio
* 9. bario
* 10. hierro
* 11. tipo: tipo de cristal:
-- 1 vidrio flotado (o sencillo) para ventanas de edificios
-- 2 vidrio no flotado para ventanas de edificios
-- 3 vidrio flotado (o sencillo) para ventanas de vehículos
-- 4 vidrio no flotado para ventanas de vehículos
-- 5 contenedores
-- 6 vajillas
-- 7 faros de vehículos
"""


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
import pickle

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import to_categorical
import tensorflow as tf
import tensorflow.keras.layers as layers
from keras.callbacks import ModelCheckpoint

"""## 1. Importación del dataset"""

column_names = ['id', 'idx_refraccion', 'sodio', 'magnesio', 'aluminio', 'silicio', 'potasio', 'calcio', 'bario', 'hierro', 'tipo']

df_glass = pd.read_csv('glass.csv',
                       decimal='.' ,
                       header= None
                       )
df_glass.columns = column_names
df_glass.info()

# Convertir la serie 'silicio' a tipo numérico con errores 'coerce'
df_glass['silicio'] = pd.to_numeric(df_glass['silicio'], errors='coerce')

# Filtrar los valores no numéricos
valores_no_numericos = df_glass.loc[df_glass['silicio'].isnull(), 'silicio']

# Mostrar los valores no numéricos
print(valores_no_numericos)

# Elimino la fila 'silicio'  113
df_glass.drop(113, inplace=True)
df_glass.info()

"""## 2. EDA&Preprocessing (2 puntos crédito, 1 punto cristales)"""

# Realiza un análisis exploratorio de los datos. ¿Puedes extraer alguna información?
#¿Hay valores duplicados? ¿Hay outliers? ¿Existen features correlacionadas entre sí?
# Realiza un preprocesado de los datos antes de pasar el siguiente apartado

df_glass.info()

df_glass.head()

#Ya no hay nulos
# Verificar duplicados en el DataFrame df
duplicados = df_glass.duplicated()
# Contar la cantidad de filas duplicadas
cantidad_duplicados = duplicados.sum()
print(cantidad_duplicados)

def estadisticos_cont(num):
    #Calculo describe
    estadisticos = num.describe().T
    #Añado la mediana
    estadisticos['median'] = num.median()
    #Reordeno para que la mediana esté al lado de la media
    estadisticos = estadisticos.iloc[:,[0,1,8,2,3,4,5,6,7]]

    return(estadisticos)

estadisticos_cont(df_glass.select_dtypes('number'))

#gráfica de la función de distribución de cada caracteristica
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
df_glass.hist(bins=50, figsize=(20,15))
plt.show()

# Obtén los datos relevantes para el análisis de outliers
data = df_glass[['idx_refraccion', 'sodio', 'magnesio', 'aluminio', 'silicio', 'potasio', 'calcio', 'bario', 'hierro', 'tipo']]

fig, axes = plt.subplots(5, 2, figsize=(12, 8))

for i, column in enumerate(data.columns):
    row = i // 2
    col = i % 2

    ax = axes[row, col]
    sns.boxplot(x=data[column], ax=ax)
    ax.set_title(f"Boxplot de {column}")

plt.tight_layout()
plt.show()

"""Hay diferentes valores claramente atípicos pero como no puedo valorar la información que aportan decido solamente eliminar el más extremo perteneciente al atributo 'magnesio' ."""

outlayer_magnesio = df_glass['magnesio'].max()
df_glass = df_glass.drop(df_glass[df_glass['magnesio'] == outlayer_magnesio].index)

# Crear un mapa de calor de correlación
sns.heatmap(df_glass.corr(), annot=True, cmap="coolwarm")  # Mapa de calor
plt.title("Mapa de calor de correlación en df_wine")
plt.show()

"""### Eliminacion de columnas y división del dataframe"""

df_glass.drop(columns=['id'], axis=1, inplace=True)
y = df_glass.pop('tipo')
X = df_glass.copy()

num_clases = len(y.unique())
print (num_clases)
print (y.unique())

"""Mapearemos la clase tipo para facilitar sun encoder posterior"""

map_tipos = {
    1: 0,
    2: 1,
    3: 2,
    5: 3,
    6: 4,
    7: 5
}

nombres_tipos = {
    0 : 'vidrio flotado para ventanas de edificios',
    1 : 'vidrio no flotado para ventanas de edificios',
    2 : ' vidrio flotado para ventanas de vehículos',
    3 : 'contenedores',
    4 : 'vajillas',
    5 : 'faros de vehículos'
}

y = y.map(map_tipos)

X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.20,random_state=42)

print (len(X_train))
print (len(X_test,))
print (len(y_train))
print (len(y_test))

"""###Normalización"""

scaler = MinMaxScaler().fit(X_train)
norm_X_train = scaler.transform(X_train)
norm_X_test = scaler.transform(X_test)

# Guardar el objeto scaler en un archivo
with open('norm_glass.pkl', 'wb') as file:
    pickle.dump(scaler, file)

"""## 3. Creación RRNN, entrenamientos, evaluaciones, predicciones y representación gráfica (4 puntos crédito, 3 puntos cristales)

### Preparación del dataset para la Red Neuronal

 Dado que no tenemos muestras con el tipo 4 , vamos a tomar la decisión de eliminarlo para facilitar el entrenamiento del modelo y reordenaremos todos los tipos empezando desde 0
"""

#Convertimos los tipos
float_X_train = norm_X_train.astype("float32")
float_X_test = norm_X_test.astype("float32")

# Convertir las etiquetas a formato categórico
y_train_encoded = to_categorical(y_train, num_classes=6)
y_test_encoded = to_categorical(y_test, num_classes=6)

num_X_train = float_X_train.shape[1]
print ("Número de Entradas:",num_X_train)
num_categorias = y_train_encoded.shape[1]
print("Número de categorías:", num_categorias)

# Crea 1 modelo de redes neuronales densamente conectada. Entrena  con el 80% de los datos y evalúa su resultado con el 20% de testing.

"""### Modelo 1"""

# Vamos a usar un modelo secuencial de Keras
# Como función de activación en las capas de entrada e intermedias vamos a usar una Relu ,
# y para la de salida una softmax que es la que más se ajusta a una clasificacón multiclase
# vamos a usar un Dropout entre las capas , es un metodo que desactiva un numero de neuronas
# de una red neuronal de forma aleatoria , este metodo ayuda a reducir el overfitting .
model= Sequential()
model.add(Dense(32,activation= 'relu',input_shape=(num_X_train,) ))
model.add(Dropout(0.2))
model.add(Dense(64,activation= 'relu' ))
model.add(Dropout(0.2))
model.add(Dense(32,activation= 'relu' ))
model.add(Dropout(0.2))
model.add(Dense(num_categorias,activation='softmax'))

# Definir el optimizador Adam con los parámetros ajustados
optimizer = Adam(lr=0.01)

# Compilar el modelo utilizando el optimizador Adam
# Como función de perdida usamos 'binary_crossentropy' que está definida para clasificación binaria
model.compile (loss='categorical_crossentropy',optimizer= optimizer, metrics=['accuracy'])

model.summary()

# Definir el callback para guardar los pesos de cada época
checkpoint = ModelCheckpoint("weights_{epoch}.h5", save_weights_only=True)

# Entrenar el modelo con el callback de ModelCheckpoint
history = model.fit(float_X_train, y_train_encoded, batch_size=32, epochs=100, verbose=0,
                    validation_data=(float_X_test, y_test_encoded), callbacks=[checkpoint])

# Obtener la precisión final del entrenamiento
train_accuracy = history.history['accuracy'][-1]

# Obtener el índice de la mejor época según la precisión de la evaluación
best_epoch = np.argmax(history.history['val_accuracy'])

# Obtener la precisión de la mejor época del entrenamiento y evaluación
train_accuracy = history.history['accuracy'][best_epoch]
test_accuracy = history.history['val_accuracy'][best_epoch]

print("Mejor época:", best_epoch+1)
print("Precisión de la mejor época del entrenamiento:", train_accuracy)
print("Precisión de la mejor época de la evaluación:", test_accuracy)

# Cargar los pesos de la mejor época
model.load_weights(f'weights_{best_epoch + 1}.h5')
# Guardar el modelo con los pesos de la mejor época
model.save('mejor_modelo.h5')
print("Modelo guardado con los pesos de la mejor época.")



# Obtener la precisión de entrenamiento y validación en cada época
train_accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']

# Obtener la pérdida de entrenamiento y validación en cada época
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Crear una figura con dos subplots en disposición de 1 fila y 2 columnas
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

# Gráfico de evolución de precisión
axs[0].plot(train_accuracy, label='Entrenamiento')
axs[0].plot(val_accuracy, label='Validación')
axs[0].set_title('Evolución de la accuracy')
axs[0].set_xlabel('Época')
axs[0].set_ylabel('Accuracy')
axs[0].legend()

# Gráfico de evolución de pérdida
axs[1].plot(train_loss, label='Entrenamiento')
axs[1].plot(val_loss, label='Validación')
axs[1].set_title('Evolución de la pérdida')
axs[1].set_xlabel('Época')
axs[1].set_ylabel('Pérdida')
axs[1].legend()

# Ajustar los subplots
plt.tight_layout()

# Mostrar los gráficos
plt.show()